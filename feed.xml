<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://koobcbc.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://koobcbc.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-11T22:10:35+00:00</updated><id>https://koobcbc.github.io/feed.xml</id><title type="html">blank</title><subtitle>MS in Applied Data Science @ University of Chicago. Research Associate in Radiation and Cellular Oncology. Data Scientist/AI Engineer at Oncosoft Inc. </subtitle><entry><title type="html">Presenting at the US-Korea Conference 2025</title><link href="https://koobcbc.github.io/blog/2025/us-korea-conference/" rel="alternate" type="text/html" title="Presenting at the US-Korea Conference 2025"/><published>2025-08-08T12:00:00+00:00</published><updated>2025-08-08T12:00:00+00:00</updated><id>https://koobcbc.github.io/blog/2025/us-korea-conference</id><content type="html" xml:base="https://koobcbc.github.io/blog/2025/us-korea-conference/"><![CDATA[<p>In August 2025, I had the opportunity to present my research at the <strong>US-Korea Conference</strong> in Atlanta, Georgia. This conference brings together researchers, professionals, and students from the United States and Korea to share scientific advances and foster international collaboration.</p> <h2 id="the-presentation">The Presentation</h2> <p>I presented a <strong>poster</strong> on our research: <strong>‚ÄúQuantitative Comparison of CT Metal Artifact Reduction (MAR) Algorithms in Radiation Oncology‚Äù</strong>. This work, conducted in collaboration with Dr. James J. Sohn and colleagues, addresses critical challenges in radiation therapy planning for patients with metal implants.</p> <h2 id="conference-experience">Conference Experience</h2> <p>The US-Korea Conference provided an excellent platform for:</p> <ul> <li><strong>International Collaboration</strong>: Connecting with researchers from both the US and Korea working on similar problems</li> <li><strong>Interdisciplinary Exchange</strong>: Learning about advances in medical physics, engineering, and related fields</li> <li><strong>Cultural Bridge</strong>: Experiencing the unique blend of American and Korean scientific communities</li> <li><strong>Professional Development</strong>: Gaining experience presenting research to an international audience</li> </ul> <p>The poster session format allowed for in-depth discussions with attendees who were particularly interested in our quantitative approach and its clinical implications.</p> <h2 id="significance">Significance</h2> <p>This research contributes to the growing body of evidence supporting the use of quantitative metrics in medical imaging evaluation. By establishing objective benchmarks, we can:</p> <ul> <li>Help radiation oncology departments make data-driven decisions about CT system selection</li> <li>Improve treatment planning accuracy for patients with metal implants</li> <li>Guide future algorithm development efforts</li> </ul> <h2 id="reflection">Reflection</h2> <p>Presenting at an international conference like the US-Korea Conference was a valuable experience that broadened my perspective on global research in medical physics. The diverse audience brought different viewpoints and questions that enriched my understanding of the work‚Äôs potential impact.</p> <p>As a Korean-American researcher, it was particularly meaningful to present at a conference that celebrates the scientific contributions of both countries. The event highlighted the importance of international collaboration in advancing medical science and improving patient care worldwide.</p> <h2 id="looking-forward">Looking Forward</h2> <p>This presentation was part of a series of conference presentations on this work, including the UChicagoGRAD Transcending Boundaries Symposium and the RSNA Annual Meeting. Each venue has provided unique opportunities to share our findings and receive feedback from different communities.</p> <p>The research continues to evolve, and we‚Äôre excited about the potential for these quantitative methods to improve clinical practice in radiation oncology.</p> <hr/> <p><em>This research is part of my ongoing work as a Research Associate in the Department of Radiation and Cellular Oncology at the University of Chicago Medicine, advised by Dr. James J. Sohn.</em></p>]]></content><author><name></name></author><category term="presentations"/><category term="presentation"/><category term="research"/><category term="medical-physics"/><category term="conference"/><summary type="html"><![CDATA[My poster presentation on CT metal artifact reduction algorithms at the US-Korea Conference in Atlanta]]></summary></entry><entry><title type="html">Winning 2nd Place at United Airlines GenAI Hackathon</title><link href="https://koobcbc.github.io/blog/2025/united-airlines-genai-hackathon/" rel="alternate" type="text/html" title="Winning 2nd Place at United Airlines GenAI Hackathon"/><published>2025-06-23T12:00:00+00:00</published><updated>2025-06-23T12:00:00+00:00</updated><id>https://koobcbc.github.io/blog/2025/united-airlines-genai-hackathon</id><content type="html" xml:base="https://koobcbc.github.io/blog/2025/united-airlines-genai-hackathon/"><![CDATA[<p>In June 2025, I had the incredible opportunity to participate in the United Airlines GenAI Hackathon, where my team and I developed an innovative agent-to-agent GenAI solution for real-time airline fault reporting. I‚Äôm thrilled to share that we won <strong>2nd place</strong> in the competition!</p> <h2 id="the-challenge">The Challenge</h2> <p>The hackathon focused on leveraging Generative AI to solve real-world problems in the aviation industry. Our team tackled the challenge of streamlining airline fault reporting‚Äîa critical process that can significantly impact operational efficiency and passenger experience.</p> <h2 id="our-solution-multi-agent-genai-system">Our Solution: Multi-Agent GenAI System</h2> <p>We architected a modular multi-agent system using <strong>LangChain</strong> and <strong>Google‚Äôs Gemini</strong> to handle multimodal inputs and automate issue triage. The system seamlessly operates across iOS, Android, and web applications, making it accessible to maintenance crews regardless of their device preference.</p> <h3 id="key-features">Key Features</h3> <p><strong>1. Specialized AI Agents</strong> Our system integrated four specialized agents, each with a distinct role:</p> <ul> <li><strong>Image Agent</strong>: Classifies and analyzes fault images uploaded by maintenance crews</li> <li><strong>Form Agent</strong>: Auto-fills structured forms based on extracted information</li> <li><strong>Submission Agent</strong>: Handles the submission workflow and data validation</li> <li><strong>Supervisor Agent</strong>: Orchestrates the entire process and ensures quality control</li> </ul> <p><strong>2. Retrieval-Augmented Generation (RAG)</strong> We implemented RAG-driven fault resolution pipelines that query knowledge bases to provide accurate, context-aware solutions. This approach significantly improved response time and accuracy compared to traditional manual processes.</p> <p><strong>3. Multimodal Input Handling</strong> The system can process various input types:</p> <ul> <li>Images of faulty equipment</li> <li>Text descriptions</li> <li>Structured form data</li> <li>Voice recordings (via transcription)</li> </ul> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/united_hackathon/IMG_9495.JPG" sizes="95vw"/> <img src="/assets/img/united_hackathon/IMG_9495.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/united_hackathon/9EEA0E05-0901-45B3-9E70-9AB9F8E6B1ED.JPG" sizes="95vw"/> <img src="/assets/img/united_hackathon/9EEA0E05-0901-45B3-9E70-9AB9F8E6B1ED.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/united_hackathon/Screenshot%202025-06-29%20at%206.24.15%20PM-480.webp 480w,/assets/img/united_hackathon/Screenshot%202025-06-29%20at%206.24.15%20PM-800.webp 800w,/assets/img/united_hackathon/Screenshot%202025-06-29%20at%206.24.15%20PM-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/united_hackathon/Screenshot%202025-06-29%20at%206.24.15%20PM.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <h2 id="demo-video">Demo Video</h2> <p>Here‚Äôs a screen recording demonstrating our multi-agent system in action:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/Screen%20Recording%202025-06-23%20at%208.36.07%20AM.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="caption"> Demo of the United Phoenix multi-agent GenAI system for airline fault reporting </div> <h2 id="technical-architecture">Technical Architecture</h2> <p>The system was built with scalability and reliability in mind:</p> <ul> <li><strong>Backend</strong>: Python with LangChain for agent orchestration</li> <li><strong>LLM</strong>: Google Gemini 2.5 for natural language understanding and generation</li> <li><strong>Frontend</strong>: Cross-platform mobile app (React Native with Expo Go) and web interface</li> <li><strong>Integration</strong>: RESTful APIs connecting all components</li> </ul> <h2 id="key-learnings">Key Learnings</h2> <p>This hackathon was an incredible learning experience that reinforced several important lessons:</p> <ol> <li><strong>Multi-Agent Systems</strong>: Breaking down complex problems into specialized agents makes systems more maintainable and effective</li> <li><strong>RAG for Domain Knowledge</strong>: Retrieval-augmented generation is crucial when dealing with domain-specific information that needs to be accurate and up-to-date</li> <li><strong>Multimodal AI</strong>: The future of AI applications lies in seamlessly handling different input types</li> <li><strong>Rapid Prototyping</strong>: The hackathon format pushed us to build a working solution quickly while maintaining code quality</li> </ol> <h2 id="impact">Impact</h2> <p>Our solution demonstrated significant potential for improving airline operations:</p> <ul> <li><strong>Faster Response Times</strong>: Automated triage reduces time from fault detection to resolution</li> <li><strong>Improved Accuracy</strong>: AI-powered classification reduces human error</li> <li><strong>Better Resource Allocation</strong>: Intelligent routing ensures issues reach the right teams quickly</li> <li><strong>Enhanced Documentation</strong>: Automated form filling ensures consistent, complete records</li> </ul> <h2 id="reflection">Reflection</h2> <p>Winning 2nd place at this hackathon was incredibly rewarding, but the real value came from the experience of working with cutting-edge GenAI technologies and solving a real-world problem. The feedback from United Airlines judges validated our approach and highlighted the potential impact of our solution.</p> <p>The hackathon also reinforced my passion for building AI systems that solve practical problems. As I continue my studies in Applied Data Science at the University of Chicago, I‚Äôm excited to apply these learnings to future projects in healthcare, biotechnology, and beyond.</p> <h2 id="presentation-slides">Presentation Slides</h2> <p>For a detailed overview of our solution, architecture, and results, check out our presentation slides:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a href="/assets/pdf/United%20Phoenix_UChicago%20&amp;%20United%20Airlines%20Hackathon%20Summer%202025.pdf" target="_blank" class="btn btn-primary"> <i class="fas fa-file-pdf"></i> View Presentation Slides (PDF) </a> </div> </div> <hr/> <p><em>If you‚Äôre interested in learning more about multi-agent systems or have questions about our implementation, feel free to reach out!</em></p>]]></content><author><name></name></author><category term="projects"/><category term="hackathon"/><category term="genai"/><category term="llm"/><category term="langchain"/><summary type="html"><![CDATA[My experience building an agent-to-agent GenAI solution for real-time airline fault reporting at the United Airlines GenAI Hackathon]]></summary></entry><entry><title type="html">Presenting at UChicagoGRAD 7th Annual Transcending Boundaries Research Symposium</title><link href="https://koobcbc.github.io/blog/2025/uchicago-transcending-boundaries-symposium/" rel="alternate" type="text/html" title="Presenting at UChicagoGRAD 7th Annual Transcending Boundaries Research Symposium"/><published>2025-05-20T12:00:00+00:00</published><updated>2025-05-20T12:00:00+00:00</updated><id>https://koobcbc.github.io/blog/2025/uchicago-transcending-boundaries-symposium</id><content type="html" xml:base="https://koobcbc.github.io/blog/2025/uchicago-transcending-boundaries-symposium/"><![CDATA[<p>In May 2025, I had the honor of presenting my research at the <strong>UChicagoGRAD 7th Annual Transcending Boundaries Research Symposium</strong> at the University of Chicago. This symposium brings together graduate students from across disciplines to showcase their innovative research and foster interdisciplinary collaboration.</p> <h2 id="the-presentation">The Presentation</h2> <p>I delivered an <strong>oral presentation</strong> on our work titled <strong>‚ÄúQuantitative Comparison of CT Metal Artifact Reduction (MAR) Algorithms in Radiation Oncology‚Äù</strong>. This research, conducted in collaboration with Dr. James J. Sohn and colleagues at the Department of Radiation and Cellular Oncology, addresses a critical challenge in radiation therapy planning.</p> <h2 id="impact-and-significance">Impact and Significance</h2> <p>This research establishes a <strong>quantitative benchmarking framework</strong> for cross-vendor MAR performance, offering objective metrics to:</p> <ul> <li>Inform system selection for radiation oncology departments</li> <li>Improve confidence in CT-based treatment planning for patients with metal implants</li> <li>Guide future development of MAR algorithms</li> </ul> <p>The work has been submitted to <em>Radiation Physics and Chemistry</em> and is currently under minor revision.</p> <h2 id="symposium-experience">Symposium Experience</h2> <p>Presenting at the Transcending Boundaries Symposium was an enriching experience. The interdisciplinary nature of the event allowed me to:</p> <ul> <li>Share our medical physics research with a diverse audience</li> <li>Receive valuable feedback from researchers across different fields</li> <li>Learn about innovative work happening across the university</li> <li>Connect with fellow graduate students passionate about advancing their fields</li> </ul> <p>The symposium highlighted the importance of clear communication and the value of presenting complex technical research in an accessible way. It was particularly rewarding to discuss how our quantitative methods could benefit clinical practice and patient care.</p> <h2 id="reflection">Reflection</h2> <p>This presentation marked an important milestone in my research journey at the University of Chicago. It reinforced my appreciation for the collaborative nature of scientific research and the importance of rigorous quantitative methods in medical physics.</p> <p>As I continue to work on this project and others in radiation oncology, I‚Äôm excited to see how these findings can contribute to improving treatment planning accuracy and ultimately, patient outcomes.</p> <hr/> <p><em>This research is part of my ongoing work as a Research Associate in the Department of Radiation and Cellular Oncology at the University of Chicago Medicine, advised by Dr. James J. Sohn.</em></p>]]></content><author><name></name></author><category term="presentations"/><category term="presentation"/><category term="research"/><category term="medical-physics"/><category term="radiation-oncology"/><summary type="html"><![CDATA[My oral presentation on quantitative comparison of CT metal artifact reduction algorithms at the UChicagoGRAD research symposium]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://koobcbc.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://koobcbc.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://koobcbc.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024 We‚Äôre introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we‚Äôre introducing Gemini 1.5 Flash: a model that‚Äôs lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We‚Äôre also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5‚Äôs 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It‚Äôs optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it‚Äôs a lighter weight model than 1.5 Pro, it‚Äôs highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it‚Äôs been trained by 1.5 Pro through a process called ‚Äúdistillation,‚Äù where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash‚Äôs availability and pricing.Over the last few months, we‚Äôve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we‚Äôve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We‚Äôve improved control over the model‚Äôs responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we‚Äôve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we‚Äôre now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do ‚Äî not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we‚Äôre also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We‚Äôre announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we‚Äôve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind‚Äôs mission to build AI responsibly to benefit humanity, we‚Äôve always wanted to develop universal AI agents that can be helpful in everyday life. That‚Äôs why today, we‚Äôre sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do ‚Äî and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we‚Äôve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we‚Äôve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we‚Äôve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they‚Äôre being used in, and respond quickly, in conversation.With technology like this, it‚Äôs easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We‚Äôve made incredible progress so far with our family of Gemini models, and we‚Äôre always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we‚Äôre able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google‚Äôs privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let‚Äôs stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[We‚Äôre sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://koobcbc.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://koobcbc.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://koobcbc.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[<h3>External Posts on Your al-folio¬†Blog</h3> <p>If you prefer publishing blog posts on medium.com or other external sources, starting version v0.5.0, <a href="https://github.com/alshedivat/al-folio">al-folio</a> lets you to display your external posts in the blog feed of your website!¬†üéâüéâ</p> <p>Configuring external sources of super simple. After upgrading to v0.5.0, just add the following section to your _config.yml:</p> <pre>external_sources:<br />  - name: medium.com  # name of the source (arbitrary string)<br />    rss_url: <a href="https://medium.com/@al-folio/feed">https://medium.com/@&lt;your-medium-username&gt;/feed</a></pre> <p>The example above adds your medium.com blog post feed as an external source. But you can add arbitrary RSS feeds as¬†sources.</p> <p>Any questions or suggestions? üëâ Start <a href="https://github.com/alshedivat/al-folio/discussions">a discussion on¬†GitHub</a>!</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b60a1d241a0a" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>